
#사전점검
-user계정으로 설치
-모든서버에 java8버전 설치
-하둡1,하둡2서버 2대는 나머지 하둡서버들의 서비스를 제어할것이므로 rsa공개키설정을 해둔다
-install이 아닌 바이너리 압축해제 설치하는 것이므로
 1대의 서버에서 먼저 전체설치하고 이를 압축해서다른서버에서 압축해제하기

#모든서버에서 user계정 생성하고 ssh키생성한다
su - user
ssh-keygen -t rsa

#하둡1과 하둡2서버 공개키설정 (이서버는 HA 네임노드임)
su - user
ssh-copy-id -i ~/.ssh/id_rsa.pub user@hadoop1~5
ssh-copy-id -i ~/.ssh/id_rsa.pub user@mesos1~3
ssh-copy-id -i ~/.ssh/id_rsa.pub user@spark1~10

#설치 바이너리 다운로드
su - user
wget http://10.10.63.63:8089/hadoop-2.9.0.tar.gz
tar zxf hadoop-2.9.0.tar.gz

#user계정 프로파일 수정
vi /home/user/.bash_profile
-----------------------------------------------------------------------------------------------
## JAVA env variables
export JAVA_HOME=/opt/jdk1.8.0_131
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
## HADOOP Env
export HADOOP_HOME=/home/user/hadoop-2.9.0
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
-----------------------------------------------------------------------------------------------

#프로파일 적용
source   /home/user/.bash_profile
clear
echo $HADOOP_OPTS

#하둡 환경변수 파일에 자바경로 지정
cat << EOF >> /home/user/hadoop-2.9.0/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/opt/jdk1.8.0_131
EOF
clear
cat /home/user/hadoop-2.9.0/etc/hadoop/hadoop-env.sh

#slaves파일작성
cat << EOF > /home/user/hadoop-2.9.0/etc/hadoop/slaves
spark1
spark2
spark3
spark4
spark5
spark6
spark7
spark8
spark9
spark10
mesos1
mesos2
mesos3
EOF
clear
cat  /home/user/hadoop-2.9.0/etc/hadoop/slaves

#core-site.xml파일작성
sed -i -e "s/<\/configuration>//g" /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<name>fs.defaultFS</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<value>hdfs://mycluster</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<name>ha.zookeeper.quorum</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<value>zk1:2181,zk2:2181,zk3:2181</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml 
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<name>hadoop.tmp.dir</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t\t<value>/mnt/hadoop/tmp</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
printf "</configuration>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml
clear
cat /home/user/hadoop-2.9.0/etc/hadoop/core-site.xml


#hdfs-site.xml파일작성
rm  /home/user/hadoop-2.9.0/etc/hadoop/hdfs-site.xml
vi /home/user/hadoop-2.9.0/etc/hadoop/hdfs-site.xml 
-------------------------------------------------------------------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>


        <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
        </property>
        <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>nn1,nn2</value>
        </property>
        <property>
                <name>dfs.namenode.rpc-address.mycluster.nn1</name>
                <value>hadoop1:8020</value>
        </property>
        <property>
                <name>dfs.namenode.rpc-address.mycluster.nn2</name>
                <value>hadoop2:8020</value>
        </property>
        <property>
                <name>dfs.namenode.http-address.mycluster.nn1</name>
                <value>hadoop1:50070</value>
        </property>
        <property>
                <name>dfs.namenode.http-address.mycluster.nn2</name>
                <value>hadoop2:50070</value>
        </property>
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://hadoop3:8485;hadoop4:8485;hadoop5:8485/mycluster</value>
        </property>
        <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
        </property>
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/home/user/.ssh/id_rsa</value>
        </property>
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
        <property>
                <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
                <value>false</value>
        </property>
</configuration>
-------------------------------------------------------------------------------------------------------------


#(생략가능)mapred-site.xml파일작성
cp /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml.template /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
sed -i -e "s/<\/configuration>//g" /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
printf "\t\t<name>mapreduce.framework.name</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
printf "\t\t<value>yarn</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml
printf "</configuration>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml 
clear
cat /home/user/hadoop-2.9.0/etc/hadoop/mapred-site.xml 


#(생략가능)yarn-site.xml파일작성
sed -i -e "s/<\/configuration>//g" /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.nodemanager.aux-services</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>mapreduce_shuffle</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.ha.enabled</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>true</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.cluster-id</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>rm-cluster</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.ha.rm-ids</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>rm1,rm2</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.hostname.rm1</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>hadoop1</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.hostname.rm2</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>hadoop2</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.zk-address</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>zk1:2181,zk2:2181,zk3:2181</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>true</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.store.class</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t<property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<name>yarn.resourcemanager.recovery.enabled</name>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t\t<value>true</value>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "\t</property>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml
printf "</configuration>\n" | tee -a /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml 
clear
cat /home/user/hadoop-2.9.0/etc/hadoop/yarn-site.xml


#하둡디렉토리생성

#exit to root
mkdir -p /mnt/hadoop/tmp
mkdir -p /mnt/hadoop/tmp/dfs/name
mkdir -p /mnt/hadoop/tmp/dfs/data

chown -R user:user /mnt/hadoop/tmp
chown -R user:user /mnt/hadoop/tmp/dfs/name
chown -R user:user /mnt/hadoop/tmp/dfs/data
su - user

#최초 수동 기동(중요)

##############################################################
Step01.저널노드기동(hadoop3, hadoop4, hadoop5)
##############################################################
su - user
cat << EOF > /home/user/hdstartup.sh
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start journalnode
EOF

chmod +x /home/user/hdstartup.sh
/home/user/hdstartup.sh

##############################################################
Step02.하둡파일시스템포멧 및 주키퍼관련ha상태초기화(hadoop1)
##############################################################
su - user
hdfs zkfc -formatZK

hdfs namenode -format

##############################################################
Step03.active namenode, resourcemgmt, historymgmt실행(hadoop1)
##############################################################
su - user
cat << EOF > /home/user/hdstartup.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start namenode
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start zkfc
sleep 1
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh start resourcemanager
sleep 1
/home/user/hadoop-2.9.0/sbin/mr-jobhistory-daemon.sh start historyserver
EOF

chmod +x /home/user/hdstartup.sh

cat << EOF > /home/user/hdstop.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/mr-jobhistory-daemon.sh stop historyserver
sleep 1
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh stop resourcemanager
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh stop zkfc
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh stop namenode
EOF

chmod +x /home/user/hdstop.sh

/home/user/hdstartup.sh

##############################################################
Step04.standby namenode 와 resourcemgmt 실행(hadoop2)
##############################################################
su - user

hdfs namenode -bootstrapStandby

cat << EOF > /home/user/hdstartup.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start namenode
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start zkfc
sleep 1
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh start resourcemanager
EOF
chmod +x hdstartup.sh

cat << EOF > /home/user/hdstop.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh stop resourcemanager
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh stop zkfc
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh stop namenode
EOF
chmod +x hdstop.sh

/home/user/hdstartup.sh

하둡1과 하둡2 서버가 제대로 동작하는지 점검
http://10.10.64.70:50070
http://10.10.64.71:50070

##############################################################
Step05.나머지 서버들에 데이터노드와 노드매니저실행(mesos1~3, spark1~10)
##############################################################
su - user
cat << EOF > /home/user/hdstartup.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh start datanode
sleep 1
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh start nodemanager
EOF
chmod +x /home/user/hdstartup.sh



cat << EOF > /home/user/hdstop.sh
#!/usr/bin/sh
/home/user/hadoop-2.9.0/sbin/yarn-daemon.sh stop  nodemanager
sleep 1
/home/user/hadoop-2.9.0/sbin/hadoop-daemon.sh stop  datanode
EOF
chmod +x /home/user/hdstop.sh


/home/user/hdstartup.sh

##############################################################
Step06.점검
##############################################################

모든 서버에서 하둡파일시스템 검색

hadoop fs -ls /


##############################################################
Step07.전체서비스 시작 및 중지
##############################################################

#하둡1번서버에서 모든서버의 서비스 중지
/home/user/hadoop-2.9.0/sbin/stop-all.sh

#하둡1번서버에서 모든서버의 서비스 실행
/home/user/hadoop-2.9.0/sbin/start-all.sh

이것을 하둡2번서버에서도 테스트



