

Single Master Cluster /w 4nodes

t1vbigdata07 : master
t1vbigdata08 : slave
t1vbigdata09 : slave
t1vbigdata10 : slave



##############################################
하둡설치 / 멀티노드 / without HA
##############################################

#사전작업(master + all slaves)

su - root


/etc/hosts -> PASS

호스트이름 FQDN으로 설정!!






useradd hadoop
passwd hadoop


su - hadoop
cat << EOF >> /home/hadoop/.bashrc
export LS_COLORS="di=00;36:fi=00;37"
EOF


ssh-keygen -t rsa


~~~~~~~~~~~~~~~~~~~~~~


exit to root


mkdir -p /opt/hadoop
chown hadoop:hadoop /opt/hadoop


systemctl disable firewalld
systemctl stop firewalld
systemctl status firewalld


#ssh(master only)

su - hadoop
ls -al


ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@t1vbigdata07.homeplusnet.co.kr
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@t1vbigdata08.homeplusnet.co.kr
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@t1vbigdata09.homeplusnet.co.kr
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@t1vbigdata10.homeplusnet.co.kr
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@localhost
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@0.0.0.0

ssh localhost
ssh t1vbigdata07.homeplusnet.co.kr
ssh t1vbigdata08.homeplusnet.co.kr
ssh t1vbigdata09.homeplusnet.co.kr
ssh t1vbigdata10.homeplusnet.co.kr


#mater + all slaves

exit # to root

cd ~
sftp -P2189 *****@x.x.x.x
cd /uploads/Users/shpark/bigdata/hadoop
get  hadoop-2.9.0.tar.gz
exit


tar xzf hadoop-2.9.0.tar.gz
cp -rf hadoop-2.9.0/* /opt/hadoop/
chown -R hadoop:hadoop /opt/hadoop/



su - hadoop
vi ~/.bashrc
-------------------------------------------------------------------------------------------
## JAVA env variables
export JAVA_HOME=/opt/jdk1.8.0_131
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
## HADOOP Env
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
-------------------------------------------------------------------------------------------

source ~/.bashrc


vi $HADOOP_HOME/etc/hadoop/hadoop-env.sh
-------------------------------------------------------------------------------------------
export JAVA_HOME=/opt/jdk1.8.0_131
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
-------------------------------------------------------------------------------------------




cd /opt/hadoop/etc/hadoop
ls


#master + all slave nodes

vi /opt/hadoop/etc/hadoop/core-site.xml
-------------------------------------------------------------------------------------------
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://t1vbigdata07.homeplusnet.co.kr:9000/</value>
  </property>
</configuration>
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/core-site.xml


#updatre hdfs-site.xml
-마스터와 슬레이드 따로 따로 설정
-replication은 data노드 숫자만큼 설정


#master only

vi /opt/hadoop/etc/hadoop/hdfs-site.xml
-------------------------------------------------------------------------------------------
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>file:/opt/hadoop/hdfs/namenode</value>
  </property>
</configuration>
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/hdfs-site.xml


#recreate namenode (hadoop@master only)
mkdir -p /opt/hadoop/hdfs/namenode
chown hadoop:hadoop -R /opt/hadoop/hdfs/namenode/
chmod 777 /opt/hadoop/hdfs/namenode
cd /opt/hadoop
ll

#all slave nodes


vi /opt/hadoop/etc/hadoop/hdfs-site.xml
-------------------------------------------------------------------------------------------
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.data.dir</name>
    <value>file:/opt/hadoop/hdfs/datanode</value>
  </property>
</configuration>
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/hdfs-site.xml


#recreate namenode (hadoop@slaves only)
mkdir -p /opt/hadoop/hdfs/datanode
chown hadoop:hadoop -R /opt/hadoop/hdfs/datanode/
chmod 777 /opt/hadoop/hdfs/datanode
cd /opt/hadoop
ll

 
#update yarn-site.xml (master + all slaves)
mapred-site.xml에서 yarn을 선택하여 셔틀 서비스를 지정한다.


vi /opt/hadoop/etc/hadoop/yarn-site.xml
-------------------------------------------------------------------------------------------
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>t1vbigdata07.homeplusnet.co.kr:8025</value>
  </property>
  <property>
    <name>yarn.resourmanager.scheduler.address</name>
    <value>t1vbigdata07.homeplusnet.co.kr:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>t1vbigdata07.homeplusnet.co.kr:8050</value>
  </property>
</configuration>
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/yarn-site.xml



#update mapred-site.xml (master + all slaves)
localhost:10020 => master:10020

cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
vi /opt/hadoop/etc/hadoop/mapred-site.xml 
-------------------------------------------------------------------------------------------
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>t1vbigdata07.homeplusnet.co.kr:10020</value>
  </property>
</configuration>
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/mapred-site.xml



#update masters and slaves file (master node only)
master 파일에는 마스터 서버만
slaves 파일에는 기존 localhost를 삭제하고 slave 호스트만


vi /opt/hadoop/etc/hadoop/mastersping
-------------------------------------------------------------------------------------------
t1vbigdata07.homeplusnet.co.kr
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/masters



vi /opt/hadoop/etc/hadoop/slaves
-------------------------------------------------------------------------------------------
t1vbigdata08.homeplusnet.co.kr
t1vbigdata09.homeplusnet.co.kr
t1vbigdata10.homeplusnet.co.kr
-------------------------------------------------------------------------------------------
cat /opt/hadoop/etc/hadoop/slaves



#format the name node(master only)

cd /opt/hadoop

hdfs namenode -format

#start the DFS & Yarn (master only)


start-dfs.sh


jps


start-yarn.sh
jps


#master> jps
namenode, secocdarynamenode, resourcemanager

#slaves> jps
datanode, nodemanager


#review

http://t1vbigdata07.homeplusnet.co.kr:8088
http:/t1vbigdata07.homeplusnet.co.kr:50070
http:/t1vbigdata07.homeplusnet.co.kr:50070/explorer.html


cd ~
echo "Welcome to Hadoop" > Readme.txt


hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoop
hdfs dfs -put /home/hadoop/Readme.txt /user/hadoop


#Now copy all files from local file system /home/hadoop/etc/hadoop/hadoop-env.sh to hadoop distributed file system using below command

hdfs dfs -put /home/hadoop/hadoop-2.9.0.tar /user/hadoop



