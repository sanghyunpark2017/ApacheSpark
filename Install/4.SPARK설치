
#mesos1~3,spark1~10에 설치한다(user)
#1대에 먼저 설치하고 나머지 서버에 복사한다

#다운로드 및 압축풀기
su - user
cd ~
wget http://10.10.63.63:8089/spark-2.2.1-bin-hadoop2.7.tgz
tar zxvf spark-2.2.1-bin-hadoop2.7.tgz

#본격 설치에 앞서 클러스터모드에서사용될 spark바이너리파일을하둡에 넣어둔다(1회 작업)
cd /home/user
hadoop fs -put spark-2.2.1-bin-hadoop2.7.tgz  /

cd /home/user/spark-2.2.1-bin-hadoop2.7
hadoop fs -put README.md  /

hadoop fs -ls /

#환경설정파일을 템플릿파일로 부터 복사하여 만든다
cd /home/user/spark-2.2.1-bin-hadoop2.7/conf/
cp  spark-env.sh.template  spark-env.sh
cp  spark-defaults.conf.template  spark-defaults.conf

#spark-env.sh 환경변수 작성
vi /home/user/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh
-----------------------------------------------------------------------------------------------
export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos.so
export SPARK_EXECUTOR_URI=hdfs://mycluster/spark-2.2.1-bin-hadoop2.7.tgz
-----------------------------------------------------------------------------------------------

#default 파일에는 구동에 필요한 정보를 넣는다
su - user
vi /home/user/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf
-----------------------------------------------------------------------------------------------
spark.master=mesos://zk://zk1:2181,zk2:2181,zk3:2181/mesos
spark.executor.uri=hdfs://mycluster/spark-2.2.1-bin-hadoop2.7.tgz
spark.deploy.recoveryMode=ZOOKEEPER
spark.deploy.zookeeper.url=zk1:2181,zk2:2181,zk3:2181
spark.deploy.zookeeper.dir=/spark
-----------------------------------------------------------------------------------------------

#slave파일작성
cat << EOF > /home/user/spark-2.2.1-bin-hadoop2.7/conf/slaves
spark1
spark2
spark3
spark4
spark5
spark6
spark7
spark8
spark9
spark10
EOF
clear
cat /home/user/spark-2.2.1-bin-hadoop2.7/conf/slaves









