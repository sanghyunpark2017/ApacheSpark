
###########################################
SPARK의 머신러닝 알고리즘 중 추천 시스템 관련
###########################################


#파이썬쉘 실행하기 전에 디스플레이 설정
#파이썬쉘 실행

su - user
export MPLBACKEND="agg"
/home/user/spark-2.2.1-bin-hadoop2.7/bin/pyspark

#대화형 액세스 지점 생성
#이전 버전에서는 SparkConf, SparkContext, SQLContxt를 사용하여 스파크와 상호작용했지만
#SPARK 2.0 부터는 SparkConf, SparkContext를 자동으로 캡슐화해 SparkSession이 처리할 수 있다.
spark = SparkSession.builder.appName("recommendationEngine").config("spark.some.config.option", "some-value").getOrCreate()

#RDD(Resilient Distributed DataSet) "data" 생성
#변경이 불가하고 서로 다른 노드에서 분리 되어 실행 될 수 있는 구조를 갖는다
data = sc.textFile("/udata.csv")


#데이터객체의 형태가 RDD임
type(data)


#데이터건수 확인
data.count()

#원본 데이터 확인(ID,제품번호,평가등급,날짜)
data.take(5)

#원본 데이터는 아이템에 대한 사용자의 평가(1~5)를 가지는 단순한 형태
-CF알고리즘(데이터를 행렬로 구성하여 유사도 계산)
-Matrix Factorization 모델
-ALS(Alternating Least Squares) 알고리즘
 : user벡터와 item벡터의 곱이 rating에 근접하도록 각 user와 item에 대한 feature vector를 정의 하는 것으로 동작
 
 
 ex) Cllaborative Filtering for the Implicit Feedback Datasets, Large-scale Parallel Collaborative Filtering for the Netflix Prize

#Spark mllib.recommendation 모듈에서 ALS, 행렬인수분해, rating함수 기능을 로드한다.
from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating

#ALS를 동작시키기 위해서 "data"에서 열구분자 ","로 구분한 후 Rating 함수를 이용하여 새로운 데이터셋 "ratings"을 만든다
#public Rating(int user, int product, double rating)
#이 떄 마지막열인 날짜를 버린다
ratings = data.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))


#데이터객체는 원본 RDD인 "data"에서 변환된 것으로 PipelinedRDD로 표기된다.
type(data)
type(ratings)


#처음 5개 데이터를 가져온다
ratings.take(5)


################
# 데이터 탐색
################

#날짜를 버린 "ratings"을 가지고 DataFrame 생성한다.
df = sqlContext.createDataFrame(ratings)

#사용자/제품/평가등급 전체 건수
df.select('user').distinct().count()
df.select('product').distinct().count()
df.select('rating').distinct().count()


#사용자/제품/평가등급 표시(15개)
df.select('user').distinct().show(15)
df.select('product').distinct().show(15)
df.select('rating').distinct().show(15)


#사용자/제품/평가등급 별 건수
df.groupBy("user").count().show()
df.groupBy("product").count().show()
df.groupBy("rating").count().show()


#사용자/제품/평가등급 별 통계
df.groupBy("user").count().select('count').describe().show()
df.groupBy("product").count().select('count').describe().show()
df.groupBy("rating").count().select('count').describe().show()



#행/열 건수
df.stat.crosstab("user", "rating").show()
df.stat.crosstab("product", "rating").show()
df.stat.crosstab("user", "product").show()


#사용자별 매긴 등급 평균/최대/최소
df.groupBy('user').agg({'rating': 'mean'}).take(5)
df.groupBy('user').agg({'rating': 'max'}).take(5)
df.groupBy('user').agg({'rating': 'min'}).take(5)
df.groupBy('user').agg({'rating': 'count'}).take(5)


#제품별 매겨진 등급 평균/최대/최소
df.groupBy('product').agg({'rating': 'mean'}).take(5)
df.groupBy('product').agg({'rating': 'max'}).take(5)
df.groupBy('product').agg({'rating': 'min'}).take(5)
df.groupBy('product').agg({'rating': 'count'}).take(5)




################################################################
# 파이썬 시각화
################################################################

#X윈도우 위에서 구동해야할 듯
#쉘시작전에 디스플레이 설정 필요함 export MPLBACKEND="agg"

import numpy as np
import matplotlib as mpl
mpl.use('agg')
import matplotlib.pyplot as plt

n_groups = 5
x = df.groupBy("rating").count().select('count')
xx = x.rdd.flatMap(lambda x: x).collect()
fig, ax = plt.subplots()

index = np.arange(n_groups)
bar_width = 1
opacity = 0.4
rects1 = plt.bar(index, xx, bar_width,alpha=opacity,color='b', label='ratings')

plt.xlabel('ratings')
plt.ylabel('Counts')
plt.title('Distribution of ratings')
plt.xticks(index + bar_width, ('1.0', '2.0', '3.0', '4.0', '5.0'))
plt.legend()
plt.tight_layout()


################################################################
# 기본 추천엔진 만들기
################################################################


#데이터셋 ratings를 randomSplist() 메소드를 사용해 무작위로 8:2 비율로 학습데이터집합과 테스트데이터집합으로 쪼갠다.
#원본데이터 : ratings
#학습데이터 : training
#테스트데이터 : test

(training, test) = ratings.randomSplit([0.8, 0.2])


ratings.count()
training.count()
test.count()

training.cache()
test.cache()


#사용할 feature vector의 size 10(default)
#값을 올리면 더 나은 모델을 만들수 있지만 계산 비용이 올라가고 오히려 prediction err가 증가할 수 있음,최적화필요
rank = 10

#반복 실행 횟수는 10
numIterations = 10


#앞서 쪼갠 training 데이터를 가지고 모델생성
model = ALS.train(training, rank, numIterations)

#Matrixfactorizationsmodel 객체가 생성되었는지 확인
type(model)


#Examine the latent features for one user
model.userFeatures().first()


#Examine the latent features for one product
model.productFeatures().first()


# For Product X, Find N Users to Sell To
model.recommendUsers(242,50)

# For User Y Find N Products to Promote
model.recommendProducts(196,10)

#Predict Single Product for Single User
model.predict(196, 242)


# Predict Multi Users and Multi Products
# Pre-Processing
pred_input = training.map(lambda x:(x[0],x[1]))

# Lots of Predictions
#Returns Ratings(user, item, prediction)
pred = model.predictAll(pred_input)

#Get Performance Estimate
#Organize the data to make (user, product) the key)
true_reorg = training.map(lambda x:((x[0],x[1]), x[2]))
pred_reorg = pred.map(lambda x:((x[0],x[1]), x[2]))

#Do the actual join
true_pred = true_reorg.join(pred_reorg)

#Need to be able to square root the Mean-Squared Error
from math import sqrt

MSE = true_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()
RMSE = sqrt(MSE)      
RMSE    #Results in 0.7629908117414474


##Test Set Evaluation
test_input = test.map(lambda x:(x[0],x[1])) 
pred_test = model.predictAll(test_input)
test_reorg = test.map(lambda x:((x[0],x[1]), x[2]))
pred_reorg = pred_test.map(lambda x:((x[0],x[1]), x[2]))
test_pred = test_reorg.join(pred_reorg)
test_MSE = test_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()
test_RMSE = sqrt(test_MSE)   
test_RMSE    #1.0145549956596238

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
http://www.learnbymarketing.com/644/recsys-pyspark-als/
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~






#원본(training) 에서 2개 가져와 봄
training.take(2)


#예측메소드 predict()를 사용
model.predict(196, 242)
model.predict(22, 377)

4.1414...
1.1248...

#또 다른 것
model.predict(94, 160)
model.predict(586, 160)
model.predict(297, 160)

4.3476...
3.2731...
3.2665...


#test 데이터셋트에서 2개(user, product) 열만 가져와서 새로운 파이프라인RDD "testdata"를 생성한다
testdata = test.map(lambda p: (p[0], p[1]))


#열이 3개에서 2개로 줄었다.
testdata.take(5)

 
#데이터셋 "testdata"의 모든 예측 값을 계산한다.
predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))


#첫 예측 다섯 개 표시
predictions.take(5)

model.predict(94, 160)
model.predict(586, 160)
model.predict(297, 160)



################################################################
# 사용자 기반 협업 필터링
################################################################

사용자에게 상위 N개 추천항목을 권유한다.


#유저당 상위 10개 권장항목을 만들 것이라는 메소드를 만든다.
recommedItemsToUsers = model.recommendProductsForUsers(10)


#유저당 상위 10개 권장항목을 생성한다.
recommedItemsToUsers.count()


#앞부분 2명의 사용자에 대한 추천사항을 본다
recommedItemsToUsers.take(2)



################################################################
# 모델 평가
################################################################

모델정합성 평가

#원래의 등급 평점과 예측을 결합해 ratesAndPreds 객체를 만든다.
ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)


#평균제곱오차(MSE:Mean Squared Error) 계산
MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
MSE

1.171170...

#평균제곱근오차(Root Mean Square Error; RMSE)
from math import sqrt
rmse = sqrt(MSE)
rmse

1.082206...


################################################################
# 모델 선택 및 하이퍼 매개변수 튜닝
################################################################

#등급RDD 객체에서 DataFrame 객체를 생성한다.
df = sqlContext.createDataFrame(ratings)


#df 객체에서 디폴트 20개 레코드를 표시한다.
type(df)
df.show()


#randomSplist()  메소드를 사용해 학습세트와 테스트세트를 무작위 8:2 비율로 생성한다.
(training, test) = df.randomSplit([0.8, 0.2])


#파라미터 tuningmodel을 실행하기 위해 필요한 모듈을 로드한다.
from pyspark.ml.recommendation import ALS


#학습데이터의 열값만을 사용한다.
myals = ALS(userCol="user", itemCol="product", ratingCol="rating")

myals
type(myals)



#als모델에 대한 기본 매개변수 설정하는 방법

myals.explainParams()



#파이프라인 객체를 생성하고 생성된  als 모델을 파이프라인의 한단계로 설정
from pyspark.ml import Pipeline

 


pipeline = Pipeline(stages=[myals])

type(pipeline)

 
#CrossValidator 및 ParamGridBuilder를 이용해 매개변수 범위를 지정한다.
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
paramMapExplicit = ParamGridBuilder().addGrid(myals.rank, [8, 12]).addGrid(myals.maxIter, [10, 15]).addGrid(myals.regParam, [1.0, 10.0]).build()

paramMap = ParamGridBuilder().addGrid(myals.rank, [8, 12]).addGrid(myals.maxIter, [10, 15]).addGrid(myals.regParam, [1.0, 10.0]).build()




#평가자 객체 설정하기
from pyspark.ml.evaluation import RegressionEvaluator



#평가메트릭을 "rmse", 열이름을 "rating"으로 한다

evaluatorR = RegressionEvaluator(metricName="rmse", labelCol="rating")


cvExplicit = CrossValidator(estimator=myals, estimatorParamMaps=paramMap, evaluator=evaluatorR, numFolds=5)

cvExplicit = CrossValidator(estimator=myals, estimatorParamMaps=grid, evaluator=evaluatorR, numFolds=5)



#fit() 메소드를 사용해 모델 생성

cvModel = cvExplicit.fit(training)


preds = cvModel.bestModel.transform(test)
preds.show()


evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")

RMSE = evaluator.evaluate(preds)
RMSE

print("Root-mean-square error = " + str(RMSE))

evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")

rmse = evaluator.evaluate(preds)

rmse

